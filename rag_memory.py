import os
import numpy as np
import faiss
import requests
from dotenv import load_dotenv
from pypdf import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferWindowMemory

from sklearn.metrics.pairwise import cosine_similarity

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
os.environ["USER_AGENT"] = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
openai_api_key = os.getenv("OPENAI_API_KEY")

class FinancialReportRAG:
    def __init__(self, input_source):
        """
        åˆå§‹åŒ– RAG æ¨¡å‹
        :param input_source: å¯ä»¥æ˜¯ PDF ç¶²å€æˆ–æœ¬åœ° PDF æ–‡ä»¶è·¯å¾‘
        """
        if input_source.startswith("http"):
            self.pdf_path = self._download_pdf(input_source)
        else:
            self.pdf_path = input_source

        self.texts_with_pages = self._load_and_process_pdf()
        self.embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
        self.index, self.texts_with_pages = self._build_faiss_index()
        self.llm = ChatOpenAI(model="gpt-4", temperature=0)

        self.memory = ConversationBufferWindowMemory(
            memory_key="history",
            k=4,  # åªè¨˜ä½æœ€è¿‘ 5 æ¬¡å°è©±
            return_messages=True
        )

        self.language = self._detect_language()
        self.retrieval_memory = {}

    def _download_pdf(self, url):
        """å¾ç¶²å€ä¸‹è¼‰ PDF ä¸¦å­˜å„²åˆ°æœ¬åœ°"""
        pdf_filename = "draft/financial_report.pdf"
        response = requests.get(url, stream=True)
        if response.status_code == 200:
            with open(pdf_filename, "wb") as f:
                for chunk in response.iter_content(chunk_size=1024):
                    f.write(chunk)
            return pdf_filename
        else:
            raise Exception(f"ä¸‹è¼‰ PDF å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}")

    def _load_and_process_pdf(self):
        """è®€å– PDF ä¸¦ä½¿ç”¨ Overlapping Window åˆ‡å‰²æ–‡æœ¬"""
        reader = PdfReader(self.pdf_path)
        texts_with_pages = []
        for i, page in enumerate(reader.pages):
            text = page.extract_text()
            if text:
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=800,
                    chunk_overlap=400,
                    separators=["\n\n", "\n", " ", ""]
                )
                chunks = text_splitter.split_text(text)
                for chunk in chunks:
                    texts_with_pages.append((i + 1, chunk))
        return texts_with_pages

    def _build_faiss_index(self):
        """å»ºç«‹ FAISS ç´¢å¼•"""
        texts = [t[1] for t in self.texts_with_pages]
        if not texts:
            raise ValueError("ç„¡æ³•å¾ PDF æå–æ–‡æœ¬ï¼Œè«‹ç¢ºèª PDF å…§å®¹æ˜¯å¦å¯è®€å–ã€‚")

        embedded_texts = self.embeddings.embed_documents(texts)
        if not embedded_texts:
            raise ValueError("åµŒå…¥ç”Ÿæˆå¤±æ•—ï¼Œè«‹ç¢ºèª OpenAI API é‡‘é‘°æ˜¯å¦æœ‰æ•ˆï¼Œæˆ–æ–‡æœ¬æ˜¯å¦å¯ç”¨ã€‚")

        embedded_texts = np.array(embedded_texts, dtype=np.float32)
        index = faiss.IndexFlatL2(embedded_texts.shape[1])
        index.add(embedded_texts)

        return index, self.texts_with_pages

    def _detect_language(self):
        """åˆ¤æ–·èªè¨€"""
        sample_text = " ".join([t[1] for t in self.texts_with_pages[:10]])
        chinese_char_count = sum(1 for char in sample_text if '\u4e00' <= char <= '\u9fff')
        return "zh" if chinese_char_count / len(sample_text) > 0.1 else "en"

    def _get_question_embedding(self, question):
        """å–å¾—å•é¡Œçš„åµŒå…¥å‘é‡"""
        return np.array(self.embeddings.embed_query(question), dtype=np.float32)

    def _translate_question(self, question):
        """å°‡å•é¡Œç¿»è­¯æˆè‹±æ–‡ï¼ˆå¦‚æœæ–‡æœ¬æ˜¯è‹±æ–‡ï¼‰"""
        if self.language == "en":
            prompt = PromptTemplate(
                input_variables=["text"],
                template="å°‡é€™å¥è©±ç¿»è­¯æˆè‹±æ–‡ï¼š{text}",
            )
            translated_question = (prompt | self.llm).invoke({"text": question}).content
            return translated_question
        else:
            return question  # å¦‚æœæ˜¯ä¸­æ–‡ï¼Œç›´æ¥è¿”å›åŸå§‹å•é¡Œ

    def retrieve_relevant_texts(self, question, k=5):
        """æª¢ç´¢æœ€ç›¸é—œçš„æ–‡æœ¬ç‰‡æ®µ"""
        question_embedding = self._get_question_embedding(question)
        distances, indices = self.index.search(np.array([question_embedding]), k)
        results = [(self.texts_with_pages[i][0], self.texts_with_pages[i][1]) for i in indices[0]]
        return results

    def is_related_to_previous_question(self, question):
        """
        åˆ¤æ–·ç•¶å‰å•é¡Œæ˜¯å¦èˆ‡ä¸Šä¸€å€‹å•é¡Œæœ‰é—œï¼Œä¸¦ç¢ºä¿é—œè¯æ€§è¶³å¤ é«˜
        """
        history = self.memory.load_memory_variables({})["history"]

        if len(history) < 2:
            return False  # æ²’æœ‰è¶³å¤ çš„å°è©±è¨˜éŒ„ï¼Œè¦–ç‚ºä¸ç›¸é—œ

        last_question = history[-2].content if hasattr(history[-2], "content") else ""
        last_answer = history[-1].content if hasattr(history[-1], "content") else ""

        # **æ­¥é©Ÿ 1ï¼šé—œéµå­—æª¢æ¸¬ï¼ˆå¢å¼·ç‰ˆï¼‰**
        related_keywords = [
            "å¢é•·", "è®ŠåŒ–", "ç›¸æ¯”", "è¶¨å‹¢", "å½±éŸ¿", "è®Šå‹•", "æˆé•·", "ä¸‹é™", "å¢åŠ ", "æ¸›å°‘",
            "æå‡", "é™ä½", "è®Šæ›´", "å¸‚å ´ä»½é¡", "ç‡Ÿæ”¶è®Šå‹•", "æ³¢å‹•"
        ]
        if any(keyword in question.lower() for keyword in related_keywords):
            return True  # è‹¥åŒ…å«é—œéµè©ï¼Œè¦–ç‚ºç›¸é—œå•é¡Œ

        # **æ­¥é©Ÿ 2ï¼šJaccard ç›¸ä¼¼åº¦æª¢æ¸¬**
        q_words = set(question.lower().split())
        last_q_words = set(last_question.lower().split())
        jaccard_similarity = len(q_words & last_q_words) / max(len(q_words | last_q_words), 1)

        if jaccard_similarity > 0.5:  # é™ä½é–¾å€¼ï¼Œå…è¨±æ›´å¯¬é¬†çš„åŒ¹é…
            return True

        # **æ­¥é©Ÿ 3ï¼šèªç¾©ç›¸ä¼¼åº¦æª¢æ¸¬ï¼ˆé¤˜å¼¦ç›¸ä¼¼åº¦ï¼‰**
        question_embedding = np.array(self.embeddings.embed_query(question), dtype=np.float32).reshape(1, -1)
        last_question_embedding = np.array(self.embeddings.embed_query(last_question), dtype=np.float32).reshape(1, -1)

        similarity_score = cosine_similarity(question_embedding, last_question_embedding)[0][0]

        return similarity_score > 0.7  # è‹¥èªç¾©ç›¸ä¼¼åº¦è¶…é 0.7ï¼Œå‰‡è¦–ç‚ºç›¸é—œ

    def generate_answer(self, question):
        """
        ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”ï¼Œä¸¦æ ¹æ“šå•é¡Œæ˜¯å¦èˆ‡éå»å°è©±ç›¸é—œæ±ºå®šæ˜¯å¦è®€å–è¨˜æ†¶
        """
        is_related = self.is_related_to_previous_question(question)  # æª¢æŸ¥å•é¡Œç›¸é—œæ€§
        history = self.memory.load_memory_variables({})["history"]

        if is_related and len(history) > 1:
            # **æ“·å–ä¸Šä¸€å€‹å•é¡ŒåŠå›ç­”**
            last_question = history[-2].content if hasattr(history[-2], "content") else ""
            last_answer = history[-1].content if hasattr(history[-1], "content") else ""

            # **å°‡ä¸Šä¸€å€‹å›ç­”çš„é—œéµè³‡è¨Šä»£å…¥æ–°å•é¡Œ**
            revised_question = f"{question} (ä¸Šä¸€å€‹å•é¡Œç‚ºï¼šã€Œ{last_question}ã€ï¼Œå…¶å›ç­”ç‚ºï¼šã€Œ{last_answer}ã€)"
            context = f"é€™æ˜¯ä½ çš„ä¸Šä¸€å€‹å•é¡ŒåŠå›ç­”ï¼Œè«‹æ ¹æ“šæ­¤è³‡è¨Šå›ç­”æ–°çš„å•é¡Œï¼Œä¸¦çµåˆæ–°çš„æª¢ç´¢å…§å®¹ä¾†æä¾›å®Œæ•´çš„å›ç­”ï¼š\n\nã€ä¸Šä¸€å€‹å•é¡Œã€‘{last_question}\nã€ä¸Šä¸€å€‹å›ç­”ã€‘{last_answer}\n\n"
            print("ğŸ”„ å•é¡Œèˆ‡å‰ä¸€å€‹å•é¡Œç›¸é—œï¼Œå°‡è®€å–ä¸Šä¸€å€‹å•é¡ŒåŠå›ç­”ï¼Œä¸¦é‡æ–°æª¢ç´¢æ–‡æœ¬ã€‚")
        else:
            revised_question = question
            context = ""
            print("ğŸ†• å•é¡Œç„¡é—œï¼Œé€²è¡Œå…¨æ–°æª¢ç´¢ã€‚")

        translated_question = revised_question if self.language == "zh" else self._translate_question(revised_question)

        # **é‡æ–°æª¢ç´¢æ–‡æœ¬**
        relevant_texts = self.retrieve_relevant_texts(translated_question, k=7)  # ğŸ”ºæ“´å¤§æª¢ç´¢ç¯„åœ
        if not relevant_texts:
            return "âš ï¸ æ–‡ä»¶æœªæä¾›ç›¸é—œè³‡è¨Šï¼Œè«‹å˜—è©¦ä¸åŒçš„å•é¡Œæˆ–åƒè€ƒå®˜æ–¹è³‡æ–™ä¾†æºã€‚"

        document_context = "\n\n".join([f"(é ç¢¼: {p}) {t}" for p, t in relevant_texts])
        source_pages = {p for p, _ in relevant_texts}
        source_pages_str = ", ".join(map(str, sorted(source_pages)))

        prompt_template = f"""
        ä½ æ˜¯ä¸€ä½åˆ†æå¸«ï¼Œè«‹æ ¹æ“šæä¾›çš„æ–‡ä»¶å…§å®¹å›ç­”å•é¡Œã€‚
    
        {context}  # æ’å…¥éå»çš„å•é¡Œèˆ‡å›ç­”ï¼ˆå¦‚æœé©ç”¨ï¼‰
    
        ã€æ–‡ä»¶å…§å®¹ã€‘
        {document_context}
    
        ã€å•é¡Œã€‘
        {revised_question}
    
        è¦å‰‡ï¼š
        - **å¦‚æœæ–°å•é¡Œèˆ‡éå»å•é¡Œç›¸é—œï¼Œè«‹åƒè€ƒéå»çš„å•é¡Œèˆ‡å›ç­”ï¼Œä¸¦æ ¹æ“šæ–°çš„æª¢ç´¢å…§å®¹è£œå……å®Œæ•´è³‡è¨Šã€‚**
        - è‹¥å…§æ–‡ä¸åŒ…å«è©³ç´°æ•¸æ“šï¼Œè«‹æ ¹æ“šå·²çŸ¥è³‡è¨Šé€²è¡Œæ¨ç†ä¸¦è£œå……ã€‚
        - è‹¥æ–°å•é¡Œèˆ‡éå»å•é¡Œç„¡é—œï¼Œè«‹å®Œå…¨ä¾æ“šç•¶å‰æ–‡ä»¶å…§å®¹å›ç­”ã€‚
        - ç›´æ¥å›ç­”å•é¡Œï¼Œä¸è¦é‡è¤‡å•é¡Œå…§å®¹ã€‚
        - åªé¡¯ç¤ºå…·é«”ç­”æ¡ˆï¼Œä¸è¦åŠ å…¥è§£é‡‹æ€§æè¿°ã€‚
        - ä½¿ç”¨å°ç£çš„ç¹é«”ä¸­æ–‡é€šé †èªæ„ï¼Œ
        - è‹¥å›ç­”æ¶‰åŠé‡‘éŒ¢å–®ä½ï¼Œè«‹ç¢ºèªå…§æ–‡ç‚ºæ–°å°å¹£æˆ–ç¾å…ƒã€‚
        """

        response = self.llm.invoke(prompt_template)

        # **æ›´æ–°å°è©±è¨˜æ†¶**
        self.memory.save_context({"question": question}, {"answer": response.content})

        return f"{response.content}\n\n  (ä¾†æºé ç¢¼: {source_pages_str})"



# æ¸¬è©¦ç¨‹å¼ç¢¼
if __name__ == "__main__":
    pdf_url = "https://s201.q4cdn.com/141608511/files/doc_financials/2025/q3/ed2a395c-5e9b-4411-8b4a-a718d192155a.pdf"
    rag = FinancialReportRAG(pdf_url)

    question1 = "è«‹å•ä¸»è¦ç‡Ÿé‹çš„å–®ä½æ˜¯å“ªå…©å€‹å–®ä½"
    print("\nğŸ” LLM å›æ‡‰ï¼š\n", rag.generate_answer(question1))

    question2 = "è«‹å•é€™å…©å€‹å–®ä½çš„æ¥­å‹™å…§å®¹ï¼Ÿ"
    print("\nğŸ” LLM å›æ‡‰ï¼š\n", rag.generate_answer(question2))

    question3 = "è«‹å•è©²å…¬å¸çš„æœªä¾†æŠ•è³‡è¨ˆåŠƒï¼Ÿ"
    print("\nğŸ” LLM å›æ‡‰ï¼š\n", rag.generate_answer(question3))

