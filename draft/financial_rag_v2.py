import os
import numpy as np
import faiss
import requests
from dotenv import load_dotenv
from pypdf import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
os.environ[
    "USER_AGENT"] = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
openai_api_key = os.getenv("OPENAI_API_KEY")

class FinancialReportRAG:
    def __init__(self, input_source):
        """
        åˆå§‹åŒ– RAG æ¨¡å‹
        :param input_source: å¯ä»¥æ˜¯ PDF ç¶²å€æˆ–æœ¬åœ° PDF æ–‡ä»¶è·¯å¾‘
        """
        if input_source.startswith("http"):
            # å¦‚æœæ˜¯ç¶²å€ï¼Œä¸‹è¼‰ PDF
            self.pdf_path = self._download_pdf(input_source)
        else:
            # å¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨
            self.pdf_path = input_source

        self.texts_with_pages = self._load_and_process_pdf()
        self.embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
        self.index, self.texts_with_pages = self._build_faiss_index()
        self.llm = ChatOpenAI(model="gpt-4", temperature=0)
        self.language = self._detect_language()


    def _download_pdf(self, url):
        """å¾ç¶²å€ä¸‹è¼‰ PDF ä¸¦å­˜å„²åˆ°æœ¬åœ°"""
        pdf_filename = "financial_report.pdf"
        response = requests.get(url, stream=True)

        if response.status_code == 200:
            with open(pdf_filename, "wb") as f:
                for chunk in response.iter_content(chunk_size=1024):
                    f.write(chunk)
            return pdf_filename
        else:
            raise Exception(f"ä¸‹è¼‰ PDF å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}")

    def _load_and_process_pdf(self):
        """è®€å– PDF ä¸¦ä½¿ç”¨ Overlapping Window åˆ‡å‰²æ–‡æœ¬"""
        reader = PdfReader(self.pdf_path)
        texts_with_pages = []

        for i, page in enumerate(reader.pages):
            text = page.extract_text()
            if text:
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=800,  # âœ… èª¿æ•´ chunk å¤§å°
                    chunk_overlap=400,  # âœ… å¢åŠ  Overlapping Windowï¼Œç¢ºä¿å…§å®¹é€£è²«
                    separators=["\n\n", "\n", " ", ""]
                )
                chunks = text_splitter.split_text(text)
                for chunk in chunks:
                    texts_with_pages.append((i + 1, chunk))

        return texts_with_pages

    def _build_faiss_index(self):
        """ç”ŸæˆåµŒå…¥å‘é‡ä¸¦å»ºç«‹ FAISS æœç´¢ç´¢å¼•"""
        texts = [t[1] for t in self.texts_with_pages]  # åªå–æ–‡æœ¬
        print(f"è™•ç†æ–‡æœ¬æ•¸é‡: {len(texts)}")  # ç¢ºä¿æœ‰æ–‡æœ¬è¼¸å…¥

        if not texts:
            raise ValueError("ç„¡æ³•å¾ PDF æå–æ–‡æœ¬ï¼Œè«‹ç¢ºèª PDF å…§å®¹æ˜¯å¦å¯è®€å–ã€‚")

        embedded_texts = self.embeddings.embed_documents(texts)
        #print(f"åµŒå…¥æ•¸æ“š: {embedded_texts}")  # ç¢ºä¿åµŒå…¥æ•¸æ“šä¸ç‚ºç©º

        if embedded_texts is None or len(embedded_texts) == 0:
            raise ValueError("åµŒå…¥ç”Ÿæˆå¤±æ•—ï¼Œè«‹ç¢ºèª OpenAI API é‡‘é‘°æ˜¯å¦æœ‰æ•ˆï¼Œæˆ–æ–‡æœ¬æ˜¯å¦å¯ç”¨ã€‚")

        embedded_texts = np.array(embedded_texts, dtype=np.float32)

        if embedded_texts.shape[0] == 0:
            raise ValueError("åµŒå…¥çŸ©é™£ç‚ºç©ºï¼Œç„¡æ³•å»ºç«‹ FAISS ç´¢å¼•ã€‚è«‹æª¢æŸ¥å‰é¢æ­¥é©Ÿæ˜¯å¦æˆåŠŸã€‚")

        # è¨­ç½® FAISS ç´¢å¼•
        dimension = embedded_texts.shape[1]
        print(f"FAISS ç´¢å¼•ç¶­åº¦: {dimension}")

        index = faiss.IndexFlatL2(dimension)
        index.add(embedded_texts)

        return index, self.texts_with_pages

    def _detect_language(self):
        """åˆ¤æ–·æ–‡æœ¬æ˜¯ä¸­æ–‡é‚„æ˜¯è‹±æ–‡"""
        sample_text = " ".join([t[1] for t in self.texts_with_pages[:10]])  # å–å‰ 10 æ®µæ–‡æœ¬ä½œç‚ºæ¨£æœ¬
        chinese_char_count = sum(1 for char in sample_text if '\u4e00' <= char <= '\u9fff')  # çµ±è¨ˆä¸­æ–‡å­—æ•¸

        # å¦‚æœä¸­æ–‡å­—æ•¸è¶…é 10%ï¼Œå‰‡åˆ¤æ–·ç‚ºä¸­æ–‡
        if chinese_char_count / len(sample_text) > 0.1:
            return "zh"  # ä¸­æ–‡
        else:
            return "en"  # è‹±æ–‡

    def _get_question_embedding(self, question):
        """å–å¾—å•é¡Œçš„åµŒå…¥å‘é‡"""
        return np.array(self.embeddings.embed_query(question), dtype=np.float32)

    def _translate_question(self, question):
        """å°‡å•é¡Œç¿»è­¯æˆè‹±æ–‡ï¼ˆå¦‚æœæ–‡æœ¬æ˜¯è‹±æ–‡ï¼‰"""
        if self.language == "en":
            prompt = PromptTemplate(
                input_variables=["text"],
                template="å°‡é€™å¥è©±ç¿»è­¯æˆè‹±æ–‡ï¼š{text}",
            )
            translated_question = (prompt | self.llm).invoke({"text": question}).content
            return translated_question
        else:
            return question  # å¦‚æœæ˜¯ä¸­æ–‡ï¼Œç›´æ¥è¿”å›åŸå§‹å•é¡Œ

    def retrieve_relevant_texts(self, question, k=3):
        """æ ¹æ“šå•é¡Œæª¢ç´¢æœ€ç›¸é—œçš„æ–‡æœ¬ç‰‡æ®µ"""
        question_embedding = self._get_question_embedding(question)
        distances, indices = self.index.search(np.array([question_embedding]), k)

        results = []
        for i in indices[0]:
            page_num, text = self.texts_with_pages[i]
            results.append((page_num, text))

        return results

    def generate_answer(self, question):
        """ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”ï¼Œä¸¦ç¢ºä¿æä¾›æ­£ç¢ºçš„ä¾†æºé ç¢¼"""
        # æ ¹æ“šèªè¨€ç¿»è­¯å•é¡Œ
        translated_question = self._translate_question(question)

        # æª¢ç´¢ç›¸é—œæ–‡æœ¬
        relevant_texts = self.retrieve_relevant_texts(translated_question)

        # å¦‚æœæ²’æœ‰æ‰¾åˆ°è¶³å¤ çš„ç›¸é—œå…§å®¹ï¼Œç›´æ¥è¿”å›å›ºå®šè¨Šæ¯
        if not relevant_texts:
            return "âš ï¸ æ–‡ä»¶æœªæä¾›ç›¸é—œè³‡è¨Šï¼Œè«‹å˜—è©¦ä¸åŒçš„å•é¡Œæˆ–åƒè€ƒå®˜æ–¹è³‡æ–™ä¾†æºã€‚"
        ranked_texts = sorted(relevant_texts, key=lambda x: len(x[1]), reverse=True)[:4]  # å–æœ€ç›¸é—œçš„3å€‹

        # æ•´ç† Context å…§å®¹
        context = "\n\n".join([f"(é ç¢¼: {p}) {t}" for p, t in relevant_texts])
        source_pages = {p for p, _ in relevant_texts}  # è¨˜éŒ„ä¾†æºé ç¢¼
        source_pages_str = ", ".join(map(str, sorted(source_pages)))

        # æ ¹æ“šèªè¨€é¸æ“‡ Prompt
        if self.language == "en":
            prompt = f"""
            You are a financial analyst. Answer the question below based ONLY on the provided document excerpts.

            Context:
            {context}

            Question: {translated_question}

            Rules:
            - If the context does not contain relevant information, respond with: "The provided document does not contain relevant information."
            - Keep the answer Concise and professional.
            - Provide an answer based on the document content.
            - Do not repeat the question.
            - Only include the answer itself without additional explanations.
            - **Translate the content into Traditional Chinese used in Taiwan.**
            
            """
        else:
            prompt = f"""
            ä½ æ˜¯ä¸€ä½è²¡å‹™åˆ†æå¸«ã€‚è«‹æ ¹æ“šæä¾›çš„æ–‡ä»¶å…§å®¹å›ç­”ä»¥ä¸‹å•é¡Œã€‚
            
            è¦å‰‡ï¼š
            **ç¿»è­¯ç‚ºè‡ºç£ä½¿ç”¨çš„ç¹é«”ä¸­æ–‡**
            å¦‚æœå…§æ–‡ä¸åŒ…å«ç›¸é—œè¨Šæ¯ï¼Œè«‹å›ç­”ï¼šâ€œæä¾›çš„æ–‡æª”ä¸åŒ…å«ç›¸é—œè¨Šæ¯ã€‚â€
            è«‹æä¾›èˆ‡æ–‡ä»¶å…§å®¹ç›¸é—œçš„å›ç­”ï¼Œ
            é¿å…é‡è¤‡ä½¿ç”¨å•é¡Œå¥ï¼Œ
            åªé¡¯ç¤ºå…·é«”å›ç­”å…§å®¹ï¼Œä¸è¦åŒ…å«è§£é‡‹æ€§æè¿°ã€‚

            å…§å®¹ï¼š
            {context}

            å•é¡Œï¼š{question}
             
        """


        response = self.llm.invoke(prompt)

        # å›å‚³ LLM å›æ‡‰ä¸¦é™„ä¸Šä¾†æºé ç¢¼
        return f"{response.content}\n\nğŸ“Œ ä¾†æºé ç¢¼: {source_pages_str}"



# æ¸¬è©¦ç¨‹å¼ç¢¼
if __name__ == "__main__":
    pdf_url = "https://s201.q4cdn.com/141608511/files/doc_financials/2025/q3/ed2a395c-5e9b-4411-8b4a-a718d192155a.pdf"
    #pdf_url = "https://www.cathayholdings.com/holdings/-/media/2703c7e79a714f51ae42d8ebd434ff09.pdf?sc_lang=zh-tw" #ä¸­æ–‡

    rag = FinancialReportRAG(pdf_url)

    question = "2025è²¡å¹´Q3ä¸»è¦ç‡Ÿæ”¶åœ‹å®¶æœ‰å“ªäº›åŠå„å¤šå°‘ç¾é‡‘ï¼Ÿ"

    # æ¸¬è©¦æª¢ç´¢çš„æ–‡æœ¬
    relevant_texts = rag.retrieve_relevant_texts(question)
    print("ğŸ” æª¢ç´¢åˆ°çš„ç›¸é—œæ–‡æœ¬ï¼š")
    for page, text in relevant_texts:
        print(f"ğŸ“„ é ç¢¼: {page}\n{text}\n")

    # æ¸¬è©¦ LLM ç”¢ç”Ÿæ‘˜è¦å›ç­”
    summary = rag.generate_answer(question)
    print("\nğŸ” LLM æ‘˜è¦å›æ‡‰ï¼š\n", summary)
